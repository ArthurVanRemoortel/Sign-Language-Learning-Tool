{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from pathlib import Path\n",
    "from time import sleep\n",
    "import skvideo.io\n",
    "import skvideo.utils\n",
    "import keras\n",
    "from keras.utils import to_categorical\n",
    "import pandas as pd\n",
    "from sl_ai.utils import clean_listdir\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: '..\\\\..\\\\ai_data\\\\vgt-all-360'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn [2], line 7\u001B[0m\n\u001B[0;32m      5\u001B[0m ROOT_PATH \u001B[38;5;241m=\u001B[39m Path(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m../../ai_data/vgt-all-360\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m      6\u001B[0m count \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[1;32m----> 7\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m j \u001B[38;5;129;01min\u001B[39;00m \u001B[43mclean_listdir\u001B[49m\u001B[43m(\u001B[49m\u001B[43mROOT_PATH\u001B[49m\u001B[43m)\u001B[49m:\n\u001B[0;32m      8\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m j\u001B[38;5;241m.\u001B[39mstartswith(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m'\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(clean_listdir(ROOT_PATH \u001B[38;5;241m/\u001B[39m j)) \u001B[38;5;241m>\u001B[39m\u001B[38;5;241m=\u001B[39m DATASET_CUTOFF:\n\u001B[0;32m      9\u001B[0m         lookup[j] \u001B[38;5;241m=\u001B[39m count\n",
      "File \u001B[1;32m~\\Desktop\\EhB\\22-23\\Final Work\\Sign-Language-Learning-Tool\\sl_ai\\utils.py:6\u001B[0m, in \u001B[0;36mclean_listdir\u001B[1;34m(path)\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mclean_listdir\u001B[39m(path: Path):\n\u001B[1;32m----> 6\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mlist\u001B[39m(\u001B[38;5;28mfilter\u001B[39m(\u001B[38;5;28;01mlambda\u001B[39;00m item: \u001B[38;5;129;01mnot\u001B[39;00m item\u001B[38;5;241m.\u001B[39mstartswith(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m), \u001B[43mos\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlistdir\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m)\u001B[49m))\n",
      "\u001B[1;31mFileNotFoundError\u001B[0m: [WinError 3] The system cannot find the path specified: '..\\\\..\\\\ai_data\\\\vgt-all-360'"
     ]
    }
   ],
   "source": [
    "DATASET_CUTOFF = 2  # Only select gestures where the dataset is >= DATASET_CUTOFF\n",
    "\n",
    "lookup = dict()\n",
    "reverselookup = dict()\n",
    "ROOT_PATH = Path('../../ai_data/vgt-all-360')\n",
    "count = 0\n",
    "for j in clean_listdir(ROOT_PATH):\n",
    "    if not j.startswith('.') and len(clean_listdir(ROOT_PATH / j)) >= DATASET_CUTOFF:\n",
    "        lookup[j] = count\n",
    "        reverselookup[count] = j\n",
    "        count += 1\n",
    "lookup"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "reverselookup"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def rescale_list(input_list, size):\n",
    "    # Does not work for lists shorter than size yet.\n",
    "    assert len(input_list) >= size\n",
    "    skip = len(input_list) // size\n",
    "    output = [input_list[i] for i in range(0, len(input_list), skip)]\n",
    "    return output[:size]\n",
    "\n",
    "def read_video(file_path: str, as_grey=False):\n",
    "    video_data = skvideo.io.vreader(file_path, as_grey=as_grey)\n",
    "    return list(video_data)\n",
    "\n",
    "def grayscale_video(video_data):\n",
    "    # Converts video data to grayscale.\n",
    "    return skvideo.utils.rgb2gray(video_data)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "x_data = []  # All frames in dataset.\n",
    "y_data = []  # Classes/Gestures of the frames as integers.\n",
    "frame_count = 0\n",
    "total_videos = 0\n",
    "for n, gesture in enumerate(clean_listdir(ROOT_PATH)): # Loop over gestures.\n",
    "    videos = clean_listdir(ROOT_PATH / gesture)\n",
    "    if len(videos) < DATASET_CUTOFF:\n",
    "        continue\n",
    "\n",
    "    total_videos += len(videos)\n",
    "    for person_video in videos: # Loop over persons.\n",
    "        count = 0\n",
    "        video_data = read_video(str(ROOT_PATH / gesture / person_video))\n",
    "        video_data = rescale_list(video_data, 50)\n",
    "        for frame in video_data:\n",
    "            x_data.append(frame)\n",
    "            count += 1\n",
    "        y_values = np.full((count, 1), lookup[gesture])\n",
    "        y_data.append(y_values)\n",
    "        frame_count += count\n",
    "\n",
    "x_data = np.array(x_data, dtype = 'float32')\n",
    "y_data = np.array(y_data)\n",
    "y_data = y_data.reshape(frame_count, 1) # Res"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import cv2\n",
    "# Image colors look distorted but this is only in the preview. The raw data is still correct.\n",
    "for i in range(0, len(lookup)):\n",
    "    img_data = x_data[i*50 + 25 , :, :]\n",
    "    img_data = img_data.astype(np.uint8)\n",
    "    plt.imshow(img_data)\n",
    "    plt.title(reverselookup[y_data[i*50+25 , 0]])\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "y_data_cat = to_categorical(y_data)\n",
    "pd.DataFrame(y_data_cat)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "x_data2 = x_data.reshape((frame_count, 360, 640, 3))\n",
    "x_data2 /= 255"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# TODO: THIS IS BAD!!!. A gesture can be split in two by the split function !!!\n",
    "# TODO: Split the data manually.\n",
    "\n",
    "x_train,x_further,y_train,y_further = train_test_split(x_data2, y_data_cat, test_size = 0.33)\n",
    "# Split further vor validation data. Skip this now since the dataset is too small to split further.\n",
    "x_validate,x_test,y_validate,y_test = train_test_split(x_further,y_further, test_size = 0.5)\n",
    "# print(len(x_data2))\n",
    "# print(len(x_train))\n",
    "# print(len(x_further))\n",
    "# print(len(y_train))\n",
    "# print(len(y_further))\n",
    "# pd.DataFrame(y_train)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from keras import layers\n",
    "from keras import models\n",
    "model=models.Sequential()\n",
    "model.add(layers.Conv2D(32, (5, 5), strides=(2, 2), activation='relu', input_shape=(360, 640, 3), name='Conv2D_1'))\n",
    "model.add(layers.MaxPooling2D((2, 2), name='MaxPooling2D_1'))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu', name='Conv2D_2'))\n",
    "model.add(layers.MaxPooling2D((2, 2), name='MaxPooling2D_2'))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu', name='Conv2D_3'))\n",
    "model.add(layers.MaxPooling2D((2, 2), name='MaxPooling2D_3'))\n",
    "model.add(layers.Flatten(name='Flatten'))\n",
    "model.add(layers.Dense(128, activation='relu', name='Dense_1'))\n",
    "model.add(layers.Dense(len(lookup), activation='softmax', name='Dense_2'))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(x_train, y_train, epochs=10, batch_size=64, verbose=1, validation_data=(x_validate, y_validate))\n",
    "\n",
    "# history = model.fit(x_train, y_train, epochs=10, batch_size=64, verbose=1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Used when trained with validation data.\n",
    "# [loss, acc] = model.evaluate(x_test,y_test,verbose=1)\n",
    "\n",
    "[loss, acc] = model.evaluate(x_further,y_further,verbose=1)\n",
    "print(\"Accuracy:\" + str(acc))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.summary()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Will give an exception if training was done without validation data. This can be ignored.\n",
    "def visualize_accuracy():\n",
    "    \"\"\"Visualize model accuracy\n",
    "    \"\"\"\n",
    "    if history:\n",
    "        plt.plot(history.history['accuracy'], label='training accuracy')\n",
    "        plt.plot(history.history['val_accuracy'], label='testing accuracy')\n",
    "        plt.title('Accuracy')\n",
    "        plt.xlabel('epochs')\n",
    "        plt.ylabel('accuracy')\n",
    "        plt.legend()\n",
    "\n",
    "def visualize_loss():\n",
    "    \"\"\"Visualizes model loss\"\"\"\n",
    "    if history:\n",
    "        plt.plot(history.history['loss'], label='training loss')\n",
    "        plt.plot(history.history['val_loss'], label='testing loss')\n",
    "        plt.title('Loss')\n",
    "        plt.xlabel('epochs')\n",
    "        plt.ylabel('loss')\n",
    "        plt.legend()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "visualize_accuracy()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "visualize_loss()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.save('vgt_v1.h5')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Should refer to a video NOT part of trained data. Ideally a video from your webcam.\n",
    "# IMPORTANT: This video needs to be 640x360 resolution and without audio.\n",
    "# If you have FFMPEG installed a video can be converted using the following command.\n",
    "#   ffmpeg -y -i <INPUT> -an -vf scale=640:360 -c:v h264 <OUTPUT>\n",
    "# -ss and -to can be used to trim parts of the video.\n",
    "#   ffmpeg -y -ss 2 -to 6.2 -i <INPUT> -an -vf scale=640:360 -c:v h264 <OUTPUT>\n",
    "# change to scale=640:360,hflip to mirror recording.\n",
    "\n",
    "# input_video = Path(r'../ai_data\\vgt-test-360\\land - verenigde staten\\VERENIGDE-STATEN.mp4')\n",
    "input_video = Path(r'../../ai_data/camera_recordings/hello.mp4')\n",
    "video_data = np.array(\n",
    "    rescale_list(\n",
    "        read_video(str(input_video)),\n",
    "        50),\n",
    "    dtype = 'float32')\n",
    "\n",
    "for i in range(0, 50, 10):\n",
    "    frame = video_data[i , :, :]\n",
    "    frame = frame.astype(np.uint8)\n",
    "    plt.imshow(frame)\n",
    "    plt.show()\n",
    "\n",
    "video_data /= 255"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "prediction = model.predict(video_data)\n",
    "classes_x = np.argmax(prediction, axis=1)\n",
    "print(classes_x)\n",
    "print('Prediction per frame:')\n",
    "for i, frame in enumerate(video_data):\n",
    "    print(f\"Frame {i}:\", reverselookup[classes_x[i]])\n",
    "print(\"----------\"*5)\n",
    "most_common_prediction = np.bincount(classes_x[10:-10]).argmax()\n",
    "print(f'Predicted that {input_video.name} is gesture \"{reverselookup[most_common_prediction]}\"')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Manually analyze the prediction\n",
    "Columns are classes/gestures, rows are each frame of the video. Values are expressed as percentages."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# pprint(reverselookup)\n",
    "prediction_percents = (prediction*100)\n",
    "# prediction_percents[prediction_percents < 5] = None\n",
    "pd.DataFrame(prediction_percents.astype(np.uint8), columns=[(v.split(\" - \")[-1]) for k, v in reverselookup.items()])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Quickly see a preview of a frame at the specified index."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "frame_to_preview = 30\n",
    "frame = video_data[frame_to_preview , :, :]\n",
    "frame *= 255\n",
    "frame = frame.astype(np.uint8)\n",
    "plt.imshow(frame)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "frame_to_preview = 35\n",
    "frame = video_data[frame_to_preview , :, :]\n",
    "frame *= 255\n",
    "frame = frame.astype(np.uint8)\n",
    "plt.imshow(frame)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
